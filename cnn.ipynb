{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro\n",
        "---\n",
        "This is a project for practicing creating a convolutional neural network. The dataset used for the model is the Open Access Series of Imaging Studies (OASIS) OASIS-1 dataset. The dataset was downloaded from: https://www.kaggle.com/datasets/ninadaithal/imagesoasis\n",
        "\n",
        "Acknowledgments: â€œData were provided by OASIS-1: Cross-Sectional: Principal Investigators: D. Marcus, R, Buckner, J, Csernansky J. Morris; P50 AG05681, P01 AG03991, P01 AG026276, R01 AG021910, P20 MH071616, U24 RR021382"
      ],
      "metadata": {
        "id": "uo-iSsAfhAvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSMRbOtdt1Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44388452-a363-4dcf-972a-387580745b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commands to retrieve and unzip data"
      ],
      "metadata": {
        "id": "sK5fcpcEuv_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/OASIS-I/archive.zip\" /content/"
      ],
      "metadata": {
        "id": "px9E6yHLTU_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip -d /content/OASIS-I_extracted"
      ],
      "metadata": {
        "id": "W7HAyYvguX8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note\n",
        "---\n",
        "Because the OASIS-I dataset is composed of hundreds of similar MRI slices for each subject, randomly splitting the slices into train/test set causes the sets to not be independent. This means that one slice of a subject may end up in the training set and the neighboring slice (which is highly similar) may end up in the testing set, meaning that the model is not generalizing, but is rather recognizing similar looking slices. This leads to inflated accuracy rates. To fix this, all slices from each subject should stay in a single set.\n",
        "\n",
        "Furthermore, the number of subjects in each category of dementia vary heavily, leading to an imbalanced dataset. This can cause the convolutional neural network to learn to predict \"Non Demented\" most of time and still get a high overall accuracy. Thus, metrics other than accuracy such as F1-score will be focused on in the testing phase. Also, since there are only 2 subjects in the \"Moderate Dementia\" class, the \"Moderate Dementia\" and \"Mild Dementia\" class were merged into a single \"Mild+ Dementia\" class for better statistical reliability."
      ],
      "metadata": {
        "id": "8e_wV_RwF-6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p /content/OASIS-I_extracted/Data/Mild+\\ Dementia"
      ],
      "metadata": {
        "id": "R24Rn4NSbCIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mv /content/OASIS-I_extracted/Data/Mild\\ Dementia/* /content/OASIS-I_extracted/Data/Mild+\\ Dementia/\n",
        "! mv /content/OASIS-I_extracted/Data/Moderate\\ Dementia/* /content/OASIS-I_extracted/Data/Mild+\\ Dementia/"
      ],
      "metadata": {
        "id": "nh6DQvYkcDtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rmdir /content/OASIS-I_extracted/Data/Mild\\ Dementia\n",
        "! rmdir /content/OASIS-I_extracted/Data/Moderate\\ Dementia"
      ],
      "metadata": {
        "id": "4mO8pGI5cag3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing stuff\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.model_selection import GroupShuffleSplit"
      ],
      "metadata": {
        "id": "fiywu9nf7_Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset\n",
        "class OASISDataset(Dataset):\n",
        "  def __init__(self, data_dir, img_transform):\n",
        "    self.img_transform = img_transform\n",
        "    self.image_paths = []\n",
        "    self.labels = []\n",
        "    self.label_names = sorted(os.listdir(data_dir))\n",
        "\n",
        "    for label_id, label_name in enumerate(self.label_names):\n",
        "      label_dir = os.path.join(data_dir, label_name)\n",
        "      images = os.listdir(label_dir)\n",
        "      for image in images:\n",
        "        image_path = os.path.join(label_dir, image)\n",
        "        self.image_paths.append(image_path)\n",
        "        self.labels.append(label_id)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image_path = self.image_paths[idx]\n",
        "    image = Image.open(image_path)\n",
        "    image = self.img_transform(image)\n",
        "    label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    return image, label\n",
        "\n",
        "  def get_label_names(self):\n",
        "    return self.label_names\n",
        "\n",
        "  def get_image_paths(self):\n",
        "    return self.image_paths\n",
        "\n",
        "data_dir = \"/content/OASIS-I_extracted/Data\"\n",
        "\n",
        "img_transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((248, 496)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "data = OASISDataset(data_dir, img_transform)"
      ],
      "metadata": {
        "id": "YPrit3PBr5-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determining the number of subjects in each class\n",
        "data_dir = \"/content/OASIS-I_extracted/Data\"\n",
        "subject_id_re = re.compile(r'(OAS1_\\d{4})')\n",
        "class_names = data.get_label_names()\n",
        "class_counts = {class_name: set() for class_name in class_names}\n",
        "for class_name in class_names:\n",
        "  class_dir = os.path.join(data_dir, class_name)\n",
        "  for name in os.listdir(class_dir):\n",
        "    found = subject_id_re.search(name)\n",
        "    if found:\n",
        "      subject_id = found.group(1)\n",
        "      class_counts[class_name].add(subject_id)\n",
        "\n",
        "for class_name, subjects in class_counts.items():\n",
        "  print(f\"{class_name}: {len(subjects)} subjects\")\n",
        "\n",
        "# Storing the number of subjects in each class into a list\n",
        "# for later use in weighted sum\n",
        "class_counts_array = []\n",
        "for class_name in class_names:\n",
        "  class_counts_array.append(len(class_counts[class_name]))"
      ],
      "metadata": {
        "id": "sSRUQQCnUJmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mild+ Dementia: 23 subjects <br>\n",
        "Non Demented: 266 subjects <br>\n",
        "Very mild Dementia: 58 subjects <br>"
      ],
      "metadata": {
        "id": "5N-LZb_3fdLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up training, validation, and testing dataloaders\n",
        "image_paths = data.get_image_paths()\n",
        "subject_ids = []\n",
        "subject_id_re = re.compile(r'(OAS1_\\d{4})')\n",
        "for path in image_paths:\n",
        "  found = subject_id_re.search(path)\n",
        "  if found:\n",
        "    subject_id = found.group(1)\n",
        "    subject_ids.append(subject_id)\n",
        "\n",
        "subject_ids = np.array(subject_ids)\n",
        "\n",
        "# Uses GroupShuffleSplit to split by subject\n",
        "gss = GroupShuffleSplit(n_splits=1, test_size=0.3)\n",
        "indices = np.arange(len(data))\n",
        "train_idx, temp_idx = next(gss.split(indices, groups=subject_ids))\n",
        "temp_subjects = subject_ids[temp_idx]\n",
        "\n",
        "gss_val_test = GroupShuffleSplit(n_splits=1, test_size=0.333)\n",
        "temp_indices = np.arange(len(temp_idx))\n",
        "val_rel_idx, test_rel_idx = next(gss_val_test.split(temp_indices, groups=temp_subjects))\n",
        "val_idx = temp_idx[val_rel_idx]\n",
        "test_idx = temp_idx[test_rel_idx]\n",
        "\n",
        "train_dataset = Subset(data, train_idx)\n",
        "validation_dataset = Subset(data, val_idx)\n",
        "test_dataset = Subset(data, test_idx)\n",
        "\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "aIUBlTYb7vaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing sample images\n",
        "def imshow(img):\n",
        "  img = img/2 + 0.5\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "  plt.show()\n",
        "\n",
        "dataiter = iter(train_dataloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "metadata": {
        "id": "x0zqgbUajQMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the cnn\n",
        "class ConvolutionalNeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn_stack = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(16, 32, 5, 1, 2),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout(0.4)\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 124 * 248, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 3)\n",
        "    )\n",
        "    dummy_input = torch.randn(1, 1, 248, 496)  # your real input size\n",
        "    output = self.cnn_stack(dummy_input)\n",
        "    print(output.shape)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cnn_stack(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = ConvolutionalNeuralNetwork()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "un4mm1KRmhzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up loss function and optimizer, using weighted loss for imbalanced dataset\n",
        "learning_rate = 1e-4\n",
        "epochs = 10\n",
        "\n",
        "class_counts_tensor = torch.tensor(class_counts_array, dtype=torch.float32)\n",
        "class_weights = 1 / class_counts_tensor\n",
        "class_weights = class_weights / torch.sum(class_weights)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "HAblXBdVGsYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the training and validation loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "  validation_loss = 0.0\n",
        "  for i, (X, y) in enumerate(train_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (X, y) in validation_dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      validation_loss += loss.item()\n",
        "\n",
        "  train_losses.append(train_loss/len(train_dataloader))\n",
        "  val_losses.append(validation_loss/len(validation_dataloader))\n",
        "  print(f'[{epoch+1}] train loss:\\t{train_losses[epoch]:.3f}')\n",
        "  print(f'[{epoch+1}] val loss:\\t{val_losses[epoch]:.3f}')\n",
        "\n",
        "print('Finished training and validation')"
      ],
      "metadata": {
        "id": "oK7VFiFPI5z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting training and validation loss\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Losses vs Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFdi28cYgeUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing training losses and validation losses to a text file\n",
        "with open('/content/drive/MyDrive/OASIS-I/train_loss.txt', 'w') as f:\n",
        "    for loss in train_losses:\n",
        "        f.write(f\"{loss}\\n\")\n",
        "\n",
        "with open('/content/drive/MyDrive/OASIS-I/val_loss.txt', 'w') as f:\n",
        "    for loss in val_losses:\n",
        "        f.write(f\"{loss}\\n\")"
      ],
      "metadata": {
        "id": "TRia1u2S3g7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/OASIS-I/cnn.pth')"
      ],
      "metadata": {
        "id": "71MRSYkoRh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the trained model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/OASIS-I/cnn.pth'))"
      ],
      "metadata": {
        "id": "oigpR0g3g3Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the testing loop\n",
        "classes = data.get_label_names()\n",
        "class_correct = {classname: 0 for classname in classes}\n",
        "class_total = {classname: 0 for classname in classes}\n",
        "overall_correct = 0\n",
        "overall_total = 0\n",
        "all_predictions = []\n",
        "all_targets = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for (X, y) in test_dataloader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    outputs = model(X)\n",
        "    _, predictions = torch.max(outputs, 1)\n",
        "    overall_total += y.size(0)\n",
        "    overall_correct += (predictions == y).sum().item()\n",
        "    all_predictions.extend(predictions.cpu().numpy())\n",
        "    all_targets.extend(y.cpu().numpy())\n",
        "    for label, prediction in zip(y, predictions):\n",
        "      if label == prediction:\n",
        "        class_correct[classes[label]] += 1\n",
        "      class_total[classes[label]] +=1\n",
        "\n",
        "overall_accuracy = 100 * float(overall_correct) / overall_total\n",
        "print(f'Overall accuracy of the network on test:images: {overall_accuracy:.2f} %')\n",
        "for classname, correct_count in class_correct.items():\n",
        "  accuracy = 100 * float(correct_count) / class_total[classname]\n",
        "  print(f'Accuracy for class: {classname:5s} is {accuracy:.2f} %')"
      ],
      "metadata": {
        "id": "2LGA1umqYaVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying classification scores and plotting the confusion matrix\n",
        "print(classification_report(all_targets, all_predictions, target_names=classes))\n",
        "cm = confusion_matrix(all_targets, all_predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "86GYuk5L-s7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stuff to Learn/Improve and Final Thoughts\n",
        "---\n",
        "Due to the class imbalance, the model overfits and has issues with accuracy.  The training and validation loss looks good for the first few epochs, but the validation loss then seems to diverge completely and causes the model to overfit by a substantial amount.\n",
        "\n",
        "Stuff to Learn/Improve:\n",
        "- Figure out how to stop model from overfitting\n",
        "- Learn ways to approach class imbalance (Undersampling, Oversampling, Focal Loss Function, ...)\n",
        "- https://arxiv.org/abs/2109.09850 - Could be a good paper to read\n",
        "- Compare with different models\n",
        "- Write better documentation and learn better coding practices\n",
        "\n",
        "Final Thoughts:\n",
        "- Is splitting by subject the correct thing to do? Am I approaching the way to prepare the data wrong?\n",
        "- How complex should the model be? How can I know what the best number of in/out channels for the Conv2d layer is, when to use dropout, how large the fully connected layer should be?\n"
      ],
      "metadata": {
        "id": "6ku7-qkzgUr4"
      }
    }
  ]
}